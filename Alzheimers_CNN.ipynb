{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasonli999/Alzheimer-s_CNN/blob/main/Alzheimers_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRKpw_PdnWWl"
      },
      "source": [
        "Credit to sentdex and his Youtube PyTorch tutorials for the basic CNN Model that we adapted. All adapted sections are marked below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80MBHoe0n67W"
      },
      "source": [
        "Import Statements are below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m3s-Y1KlzJlI"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-Rs5uKMTOlnA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import time\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngytoICYoAC8"
      },
      "source": [
        "Checks to see if a GPU is available for the model to run on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EvIuhjmXW2l"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "print(f'Device is: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFULlcsEoLqP"
      },
      "source": [
        "Downloads and unzips the Kaggle dataset into the local directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a10SHADpMjZp"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n",
        "!kaggle datasets download -d uraninjo/augmented-alzheimer-mri-dataset\n",
        "!unzip \"/content/augmented-alzheimer-mri-dataset.zip\"\n",
        "#downloads and unzips the dataset from Kaggle. Note that the kaggle.json API Token will need to be uploaded to the Colab for this to work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HCsv2KUoQ0T"
      },
      "source": [
        "folder_to_array() adapted from Sentdex. Functions to pre-process the data. folder_to_array() is mainly used in the model to turn the images into numpy arrays and to classify the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fgK5Avq5Ni9V"
      },
      "outputs": [],
      "source": [
        "\n",
        "def folder_to_csv(folder, classification, save = False):\n",
        "    \"\"\"Folder is a folder path, classification is the string classifiction. Save is a boolean\n",
        "    folder_to_csv() returns a csv with the path of the images in the first column and the classification in the second column\n",
        "    setting 'save' to true saves the csv to your drive with the name of the file as the name of the classification variable\"\"\"\n",
        "    file_names = []\n",
        "    for picture in os.listdir(folder):\n",
        "      path = os.path.join(folder, picture)\n",
        "      file_names.append([path, classification])\n",
        "      df = pd.DataFrame(file_names, columns = [\"File Path\", \"Classification\"])\n",
        "    if save == True:\n",
        "        name = f'{classification}.csv'\n",
        "        csv = df.to_csv(name, index = False)\n",
        "    return df.to_csv(index = False)\n",
        "\n",
        "def folder_to_array(folder, vectorlocation, image_size, save = False):\n",
        "    \"\"\"folder is a folder path, image_size is the image size of the post-processed picture, vectorlocation is the index of the one-hot vector\n",
        "    folder_to_array() returns all of the images in the folder specified into an 2d array with column1 as the image array and column2 as the\n",
        "    one-hot vector representing the data classification\"\"\"\n",
        "    training_data = []\n",
        "    for picture in tqdm(os.listdir(folder)):\n",
        "        if \"jpg\" in picture:\n",
        "            path = os.path.join(folder, picture)\n",
        "            img = cv2.imread(path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            img = cv2.resize(img, (image_size, image_size))\n",
        "            training_data.append([np.array(img), np.eye(4)[vectorlocation]]) #<- second index is the one-hot vector\n",
        "    training_data = np.array(training_data, list)\n",
        "    if save == True:\n",
        "        np.save(folder, training_data)\n",
        "    return training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLKYddvbohMN"
      },
      "source": [
        "Calls the folder_to_array() functions and creates the training and testing data arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGp_F4EENuBM"
      },
      "outputs": [],
      "source": [
        "nondemented_array = folder_to_array(\"/content/AugmentedAlzheimerDataset/NonDemented\", 0, 100)\n",
        "verymilddemented_array = folder_to_array(\"/content/AugmentedAlzheimerDataset/VeryMildDemented\", 1, 100)\n",
        "milddemented_array = folder_to_array(\"/content/AugmentedAlzheimerDataset/MildDemented\", 2, 100)\n",
        "moderatedemented_array = folder_to_array(\"/content/AugmentedAlzheimerDataset/ModerateDemented\", 3, 100)\n",
        "#creates the arrays from the unzipped Kaggle data located in the Colab\n",
        "\n",
        "train_size = int(min(len(nondemented_array), len(verymilddemented_array), len(milddemented_array), len(moderatedemented_array)))\n",
        "#the size of each sample size in the training batch is half of the minimum sample size\n",
        "\n",
        "train_size = int(train_size * 0.2) #try about ~20% of data in training instead\n",
        "\n",
        "training_data = np.concatenate((nondemented_array[0: train_size], verymilddemented_array[0: train_size], milddemented_array[0: train_size], moderatedemented_array[0: train_size]))\n",
        "testing_data = np.concatenate((nondemented_array[train_size:], verymilddemented_array[train_size:], milddemented_array[train_size:], moderatedemented_array[train_size:]))\n",
        "#creates the training data by concatenating all of the arrays of length train_size together, creates testing data by concatenating the rest of the data\n",
        "#generally we want a smaller portion of the data to be a part of the training dataset to avoid overfitting\n",
        "np.random.shuffle(training_data)\n",
        "np.random.shuffle(testing_data)\n",
        "#Shuffling and randomizing the data\n",
        "\n",
        "training_data = np.array(training_data)\n",
        "testing_data = np.array(testing_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1g4myGMoq2l"
      },
      "source": [
        "Adapted from Sentdex. The framework for the CNN, which consists of Convolutional layers for processing the image, Max pooling layers which downscales the 2D convolutional data into 1D, and the Fully connected layers, which are the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ahwxkZsIOOBH"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #optimize number of layers, hypertune parameters\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
        "        self.pool1 = nn.MaxPool2d((2, 2))\n",
        "        self.pool2 = nn.MaxPool2d((2, 2))\n",
        "        self.pool3 = nn.MaxPool2d((2, 2))\n",
        "        self.fc1 = nn.Linear(10368, 10368) #size fo the fully connected layer was determined by the size of the pictures, which are 100 x 100 pixels\n",
        "        self.fc2 = nn.Linear(10368, 4)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = x.flatten(start_dim=1) # flattening out\n",
        "        #print(x.shape)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "  \n",
        "#net.forward(torch.randn(1, 1, 100, 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPpESEFxpNw7"
      },
      "source": [
        "Adapted from Sentdex. Transforms the numpy arrays into Pytorch tensors. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKfjXx61O2LU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "training_picture = torch.Tensor([i[0] for i in training_data]).view(-1, 100, 100)\n",
        "training_picture = training_picture/255.0\n",
        "training_class = torch.Tensor([i[1] for i in training_data])\n",
        "\n",
        "testing_picture = torch.Tensor([i[0] for i in testing_data]).view(-1, 100, 100)\n",
        "testing_picture = testing_picture/255.0\n",
        "testing_class = torch.Tensor([i[1] for i in testing_data])\n",
        "\n",
        "#Warning: \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
        "#How to fix/make more efficent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6EA_5x7pS8f"
      },
      "source": [
        "Adapted from Sentdex. The training and testing function. fwd_pass is used to pass the training data through the CNN and adjust for loss. The fwd_test function finds a random set of data of size size to test the CNN on."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fwd_pass(net, training_data, testing_data, loss_function, optimizer, train=False):\n",
        "  #trains the function\n",
        "  if train:\n",
        "    net.zero_grad()\n",
        "  outputs = net(training_data)\n",
        "  matches = [torch.argmax(i) == torch.argmax(j) for i, j in zip(outputs, testing_data)]\n",
        "  acc = matches.count(True)/len(matches)\n",
        "  loss = loss_function(outputs, testing_data)\n",
        "\n",
        "  if train:\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  return acc, loss\n",
        "\n",
        "def fwd_test(net, loss_function, optimizer, size = 32):\n",
        "  #size is the size of the random set of data that is used for the testing function\n",
        "  random_start = np.random.randint(len(testing_picture) - size)\n",
        "  test_pic, test_class = testing_picture[random_start: random_start + size], testing_class[random_start: random_start + size]\n",
        "  with torch.no_grad():\n",
        "    val_acc, val_loss = fwd_pass(net, test_pic.view(-1, 1, 100, 100).to(device), test_class.to(device), loss_function, optimizer)\n",
        "  return val_acc, val_loss\n"
      ],
      "metadata": {
        "id": "B-FZL2LQPi0O"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted from Sentdex. Rewritten by me to used Pandas. Train() trains the CNN with the passed arguments and writes on a log file with all of the testing data. create_acc_loss_graph() creates a graph of the training data accuracy vs. the testing accuracy and the training loss vs. the testing loss."
      ],
      "metadata": {
        "id": "03039V2ksaYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(MODEL_NAME = f\"model-{int(time.time())}\", BATCH_SIZE = 50, EPOCHS = 5, learning_rate = 0.00008):\n",
        "  \"\"\"train() trains and tests the data and appends the data from the results into a csv file, with each training session identified using time.time()\"\"\"\n",
        "  net = Net().to(device)\n",
        "  optimizer = optim.Adam(net.parameters(), lr = learning_rate) #optimize loss rate\n",
        "  loss_function = nn.MSELoss()\n",
        "  labelbool = os.path.exists(\"/content/model.csv\")\n",
        "  with open(\"model.csv\", \"a\") as f:\n",
        "    if labelbool == False: #checks to see if file exists and if not, append the labels to the top\n",
        "      listlabels = [\"Model Name\", \"Time\", \"Train Accuracy\", \"Train Loss\", \"Test Accuracy\", \"Test Loss\"]\n",
        "      writer_object = csv.writer(f)\n",
        "      writer_object.writerow(listlabels) #appends labels to top of csv\n",
        "    for epoch in range(EPOCHS):\n",
        "      for i in tqdm(range(0, len(training_picture), BATCH_SIZE)):\n",
        "          batch_pic = training_picture[i: i + BATCH_SIZE].view(-1, 1, 100, 100)\n",
        "          batch_class = training_class[i: i + BATCH_SIZE]\n",
        "          batch_pic, batch_class = batch_pic.to(device), batch_class.to(device)\n",
        "\n",
        "          acc, loss = fwd_pass(net, batch_pic, batch_class, loss_function, optimizer, True)\n",
        "          if i % 5 == 0: #sample rate, how many iterations til the network is tested\n",
        "            val_acc, val_loss = fwd_test(net, loss_function, optimizer, size = 100) #<- change size for different size of testing sample\n",
        "            listvars = [MODEL_NAME, round(time.time(), 3), round(float(acc), 2), round(float(loss), 4), round(float(val_acc), 2), round(float(val_loss), 4)]\n",
        "            writer_object = csv.writer(f)\n",
        "            writer_object.writerow(listvars) #appends data to bottom of csv file\n",
        "      print(f'Epoch {epoch + 1} of {EPOCHS}')\n",
        "  \n",
        "  return MODEL_NAME\n",
        "\n",
        "matplotlib.style.use(\"ggplot\")\n",
        "\n",
        "def create_acc_loss_graph(model_name, graph = True):\n",
        "  \"\"\"create_acc_loss_graph creates a graph of the train vs. test accuracy and the train vs. test loss of a given model name.\"\"\"\n",
        "  try:\n",
        "    df = pd.read_csv(\"/content/model.csv\")\n",
        "    df = df.loc[df[\"Model Name\"] == model_name] #gets only the data associated with the model name\n",
        "  except:\n",
        "    raise Exception(\"Did not load model csv in yet\")\n",
        "  \n",
        "  if graph:\n",
        "    \n",
        "    fig = plt.figure()\n",
        "    ax1 = plt.subplot2grid((2, 1), (0, 0))\n",
        "    ax2 = plt.subplot2grid((2, 1), (1, 0), sharex = ax1)\n",
        "\n",
        "    df[\"Test Accuracy\"].plot(ax = ax1, title = model_name, legend = True)\n",
        "    df[\"Train Accuracy\"].plot(ax = ax1, legend = True)\n",
        "    df[\"Train Loss\"].plot(ax = ax2, legend = True)\n",
        "    df[\"Test Loss\"].plot(ax = ax2, legend = True)\n",
        "    \n",
        "    ax1.set_xlabel(\"Time\")\n",
        "    ax1.set_ylabel(\"Accuracy\")\n",
        "    ax2.set_xlabel(\"Time\")\n",
        "    ax2.set_ylabel(\"Loss\")\n",
        "\n",
        "    plt.show()\n",
        "  \n",
        "  if graph == False:\n",
        "    return times, accuracy, val_accs, losses, val_losses"
      ],
      "metadata": {
        "id": "TFdyY861T9OV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test_params() is used to test different parameters on its affect on accuracy for epochs, loss rate, and batch size. It outputs a graph of the testing variable vs. the accuracy. "
      ],
      "metadata": {
        "id": "J0m85ATzsy_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_params(BATCH_SIZE, EPOCHS, learning_rate, graph = True):\n",
        "  if graph:\n",
        "    fig = plt.figure()\n",
        "    ax = plt.subplot2grid((2, 1), (0, 0))\n",
        "\n",
        "  if type(EPOCHS) == list and type(learning_rate) == float and type(BATCH_SIZE) == int:\n",
        "    list_epochs = []\n",
        "    list_accuracy = []\n",
        "    for epoch in EPOCHS:\n",
        "      print(f'Testing Epoch: {epoch}')\n",
        "      list_epochs.append(epoch)\n",
        "      MODEL_NAME = f\"model-{int(time.time())}\"\n",
        "      print(f'Model Name {MODEL_NAME}')\n",
        "      train(MODEL_NAME, BATCH_SIZE, epoch, learning_rate)\n",
        "      times, accuracy, val_accs, losses, val_losses = create_acc_loss_graph(MODEL_NAME, False)\n",
        "      list_ind = len(times) - int(len(times) * .1)\n",
        "      val_accuracy = val_accs[list_ind:]\n",
        "      val_average = sum(val_accuracy)/len(val_accuracy)\n",
        "      list_accuracy.append(val_average)\n",
        "    if graph:\n",
        "      ax.plot(list_epochs, list_accuracy)\n",
        "      ax.set_xlabel(\"Epochs\")\n",
        "      ax.set_ylabel(\"Accuracy\")\n",
        "    return list_epochs, list_accuracy\n",
        "  \n",
        "  elif type(EPOCHS) == int and type(learning_rate) == list and type(BATCH_SIZE) == int:\n",
        "    list_learning_rate = []\n",
        "    list_accuracy = []\n",
        "    for lr in learning_rate:\n",
        "      print(f'Testing Loss Rate: {lr}')\n",
        "      list_learning_rate.append(lr)\n",
        "      MODEL_NAME = f\"model-{int(time.time())}\"\n",
        "      print(f'Model Name {MODEL_NAME}')\n",
        "      train(MODEL_NAME, BATCH_SIZE, EPOCHS, lr)\n",
        "      times, accuracy, val_accs, losses, val_losses = create_acc_loss_graph(MODEL_NAME, False)\n",
        "      list_ind = len(times) - int(len(times) * .1)\n",
        "      val_accuracy = val_accs[list_ind:]\n",
        "      val_average = sum(val_accuracy)/len(val_accuracy)\n",
        "      list_accuracy.append(val_average)\n",
        "    if graph:\n",
        "      ax.plot(list_learning_rate, list_accuracy)\n",
        "      ax.set_xlabel(\"Loss Rates\")\n",
        "      ax.set_ylabel(\"Accuracy\")\n",
        "    return list_learning_rate, list_accuracy\n",
        "  \n",
        "  elif type(EPOCHS) == int and type(learning_rate) == float and type(BATCH_SIZE) == list:\n",
        "    list_batch_size = []\n",
        "    list_accuracy = []\n",
        "    for size in BATCH_SIZE:\n",
        "      print(f'Testing Batch Size: {size}')\n",
        "      list_batch_size.append(size)\n",
        "      MODEL_NAME = f\"model-{int(time.time())}\"\n",
        "      print(f'Model Name {MODEL_NAME}')\n",
        "      train(MODEL_NAME, size, EPOCHS, learning_rate)\n",
        "      times, accuracy, val_accs, losses, val_losses = create_acc_loss_graph(MODEL_NAME, False)\n",
        "      list_ind = len(times) - int(len(times) * .1)\n",
        "      val_accuracy = val_accs[list_ind:]\n",
        "      val_average = sum(val_accuracy)/len(val_accuracy)\n",
        "      list_accuracy.append(val_average)\n",
        "    if graph:\n",
        "      ax.plot(list_batch_size, list_accuracy)\n",
        "      ax.set_xlabel(\"Batch Sizes\")\n",
        "      ax.set_ylabel(\"Accuracy\")\n",
        "    return list_batch_size, list_accuracy\n",
        "\n",
        "  elif type(EPOCHS) == int and type(learning_rate) == float and type(BATCH_SIZE) == int:\n",
        "    MODEL_NAME = f\"model-{int(time.time())}\"\n",
        "    print(f'Model Name {MODEL_NAME}')\n",
        "    train(MODEL_NAME, BATCH_SIZE, EPOCHS, learning_rate)\n",
        "    times, accuracy, val_accs, losses, val_losses = create_acc_loss_graph(MODEL_NAME, False)\n",
        "    list_ind = len(times) - int(len(times) * .1)\n",
        "    val_accuracy = val_accs[list_ind:]\n",
        "    val_average = sum(val_accuracy)/len(val_accuracy)\n",
        "    return val_average\n",
        "  \n",
        "  else: \n",
        "    raise TypeError(\"Please Input the Correct Types\")"
      ],
      "metadata": {
        "id": "sZhchpLbj2Hu"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimal learning rate is approximately 0.00006 per current testing\n",
        "epoch_list = [1, 3, 5, 10]\n",
        "test_params(50, 30, 0.00006, False)"
      ],
      "metadata": {
        "id": "chbnWBzMtrYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_acc_loss_graph(\"model-1666933572\", True)"
      ],
      "metadata": {
        "id": "dL0yPzQj-9BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "nHBUMn3Stl8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "6kVU45SVtlhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "iHhK-FqFtcsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "EEW66hkAtk-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "W320hdQctgnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Old Functions Below For Reference:"
      ],
      "metadata": {
        "id": "LqGQY-pCtIUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "WQvy_gKSO4IM"
      },
      "outputs": [],
      "source": [
        "#Adapted from Sentdex\n",
        "\n",
        "testing_picture.to(device)\n",
        "testing_class.to(device)\n",
        "\n",
        "def train(net, BATCH_SIZE = 100, EPOCHS = 10): #optimize parametes\n",
        "  optimizer = optim.Adam(net.parameters(), lr = 0.00008) #optimize loss rate\n",
        "  loss_function = nn.MSELoss()\n",
        "  for epoch in range(EPOCHS):\n",
        "      for i in tqdm(range(0, len(training_picture), BATCH_SIZE)):\n",
        "          batch_pic = training_picture[i: i + BATCH_SIZE].view(-1, 1, 100, 100)\n",
        "          batch_class = training_class[i: i + BATCH_SIZE]\n",
        "          \n",
        "          batch_pic, batch_class = batch_pic.to(device), batch_class.to(device) \n",
        "          \n",
        "          net.zero_grad()\n",
        "          outputs = net(batch_pic)\n",
        "          loss = loss_function(outputs, batch_class)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "      print(f'Epoch: {epoch}, Loss: {loss}')\n",
        "\n",
        "testing_picture = testing_picture[0:5000]\n",
        "testing_class = testing_class[0:5000]\n",
        "\n",
        "def test(net):\n",
        "  correct, total = 0, 0\n",
        "  with torch.no_grad():\n",
        "      for i in tqdm(range(len(testing_picture))):\n",
        "          real_class = torch.argmax(testing_class[i]).to(device)\n",
        "          net_out = net(testing_picture[i].view(-1, 1, 100, 100).to(device))[0]\n",
        "          predicted_class = torch.argmax(net_out)\n",
        "          #print(f'{predicted_class}, {real_class}')\n",
        "          if predicted_class == real_class:\n",
        "              correct += 1\n",
        "          total += 1\n",
        "  print(correct, total)\n",
        "  return(round(correct/total, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ilRryfmPfyz"
      },
      "outputs": [],
      "source": [
        "net = Net().to(device)\n",
        "train(net, BATCH_SIZE = 100, EPOCHS = 5)\n",
        "print(test(net))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLb9Fgjz4PtM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss_rate_test1 = [0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
        "loss_rate_test2 = [0.00008, 0.0001, 0.00015, 0.0002, 0.00025]\n",
        "accuracy_list = []\n",
        "for loss in loss_rate_test2:\n",
        "  net1 = Net().to(device)\n",
        "  train(net1, BATCH_SIZE = 100, EPOCHS = 3, loss_rate = loss)\n",
        "  accuracy_list.append(test(net1))\n",
        "\n",
        "plt.plot(loss_rate_test2, accuracy_list)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Loss Rate\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}