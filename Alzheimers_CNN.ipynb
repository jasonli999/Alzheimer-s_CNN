{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasonli999/Alzheimer-s_CNN/blob/main/Alzheimers_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRKpw_PdnWWl"
      },
      "source": [
        "Credit to sentdex and his Youtube PyTorch tutorials for the basic CNN Model that we adapted. Credit to Ruben Winastwan's article \"Hyperparameter Tuning of Neural Networks with Optuna and PyTorch\" on *Towards Data Science* for the Optuna tutorial that we adapted. All adapted sections are marked below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80MBHoe0n67W"
      },
      "source": [
        "Import Statements are below:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "pip install optuna"
      ],
      "metadata": {
        "id": "8BNewJ7aNYmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3s-Y1KlzJlI"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Rs5uKMTOlnA"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import os \n",
        "import csv\n",
        "import cv2\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngytoICYoAC8"
      },
      "source": [
        "Checks to see if a GPU is available for the model to run on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EvIuhjmXW2l"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "print(f'Device is: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFULlcsEoLqP"
      },
      "source": [
        "Downloads and unzips the Kaggle dataset into the local directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a10SHADpMjZp"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n",
        "!kaggle datasets download -d uraninjo/augmented-alzheimer-mri-dataset\n",
        "!unzip \"/content/augmented-alzheimer-mri-dataset.zip\"\n",
        "#downloads and unzips the dataset from Kaggle. Note that the kaggle.json API Token will need to be uploaded to the Colab for this to work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HCsv2KUoQ0T"
      },
      "source": [
        "folder_to_array() adapted from Sentdex. Functions to pre-process the data. folder_to_array() is mainly used in the model to turn the images into numpy arrays and to classify the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgK5Avq5Ni9V"
      },
      "outputs": [],
      "source": [
        "\n",
        "def folder_to_csv(folder, classification, save = False):\n",
        "    \"\"\"Folder is a folder path, classification is the string classifiction. Save is a boolean\n",
        "    folder_to_csv() returns a csv with the path of the images in the first column and the classification in the second column\n",
        "    setting 'save' to true saves the csv to your drive with the name of the file as the name of the classification variable\"\"\"\n",
        "    file_names = []\n",
        "    for picture in os.listdir(folder):\n",
        "      path = os.path.join(folder, picture)\n",
        "      file_names.append([path, classification])\n",
        "      df = pd.DataFrame(file_names, columns = [\"File Path\", \"Classification\"])\n",
        "    if save == True:\n",
        "        name = f'{classification}.csv'\n",
        "        csv = df.to_csv(name, index = False)\n",
        "    return df.to_csv(index = False)\n",
        "\n",
        "def folder_to_array(folder, vectorlocation, image_size, save = False):\n",
        "    \"\"\"folder is a folder path, image_size is the image size of the post-processed picture, vectorlocation is the index of the one-hot vector\n",
        "    folder_to_array() returns all of the images in the folder specified into an 2d array with column1 as the image array and column2 as the\n",
        "    one-hot vector representing the data classification\"\"\"\n",
        "    training_data = []\n",
        "    for picture in tqdm(os.listdir(folder)):\n",
        "        if \"jpg\" in picture:\n",
        "            path = os.path.join(folder, picture)\n",
        "            img = cv2.imread(path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            img = cv2.resize(img, (image_size, image_size))\n",
        "            training_data.append([np.array(img), np.eye(4)[vectorlocation]]) #<- second index is the one-hot vector\n",
        "    training_data = np.array(training_data, list)\n",
        "    if save == True:\n",
        "        np.save(folder, training_data)\n",
        "    return training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLKYddvbohMN"
      },
      "source": [
        "Calls the folder_to_array() functions and creates the training and testing data arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGp_F4EENuBM"
      },
      "outputs": [],
      "source": [
        "def create_data(size):\n",
        "  \"\"\"create_data creates the training and testin data used in the CNN by calling folder_to_array() and returning randomized training and testing arrays.\n",
        "  size is the percentage of the minimum dataset size that the training data will take.\"\"\"\n",
        "  nondemented_array = folder_to_array(\"/content/AugmentedAlzheimerDataset/NonDemented\", 0, 100)\n",
        "  verymilddemented_array = folder_to_array(\"/content/AugmentedAlzheimerDataset/VeryMildDemented\", 1, 100)\n",
        "  milddemented_array = folder_to_array(\"/content/AugmentedAlzheimerDataset/MildDemented\", 2, 100)\n",
        "  moderatedemented_array = folder_to_array(\"/content/AugmentedAlzheimerDataset/ModerateDemented\", 3, 100)\n",
        "  #creates the arrays from the unzipped Kaggle data located in the Colab\n",
        "\n",
        "  train_size = int(min(len(nondemented_array), len(verymilddemented_array), len(milddemented_array), len(moderatedemented_array)))\n",
        "  #the size of each sample size in the training batch is half of the minimum sample size\n",
        "\n",
        "  train_size = int(train_size * size) #try about ~20% of data in training\n",
        "\n",
        "  training_data = np.concatenate((nondemented_array[0: train_size], verymilddemented_array[0: train_size], milddemented_array[0: train_size], moderatedemented_array[0: train_size]))\n",
        "  testing_data = np.concatenate((nondemented_array[train_size:], verymilddemented_array[train_size:], milddemented_array[train_size:], moderatedemented_array[train_size:]))\n",
        "  #creates the training data by concatenating all of the arrays of length train_size together, creates testing data by concatenating the rest of the data\n",
        "\n",
        "  np.random.shuffle(training_data)\n",
        "  np.random.shuffle(testing_data)\n",
        "  #Shuffling and randomizing the data\n",
        "\n",
        "  training_data = np.array(training_data)\n",
        "  testing_data = np.array(testing_data)\n",
        "\n",
        "  return training_data, testing_data\n",
        "\n",
        "training_data, testing_data = create_data(0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1g4myGMoq2l"
      },
      "source": [
        "Adapted from Sentdex. The framework for the CNN, which consists of Convolutional layers for processing the image, Max pooling layers which downscales the 2D convolutional data into 1D, and the Fully connected layers, which are the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahwxkZsIOOBH"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # optimize number of layers, hypertune parameters\n",
        "        # look at parameters of Conv2D \n",
        "        # look at parameters of MaxPool2D  \n",
        "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
        "        self.pool1 = nn.MaxPool2d((2, 2))\n",
        "        self.pool2 = nn.MaxPool2d((2, 2))\n",
        "        self.pool3 = nn.MaxPool2d((2, 2))\n",
        "        self.fc1 = nn.Linear(10368, 10368) #size fo the fully connected layer was determined by the size of the pictures, which are 100 x 100 pixels\n",
        "        self.fc2 = nn.Linear(10368, 4) # must be hard-set, based on size image and classes \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = x.flatten(start_dim=1) # flattening out\n",
        "        #print(x.shape)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "  \n",
        "#net.forward(torch.randn(1, 1, 100, 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPpESEFxpNw7"
      },
      "source": [
        "Adapted from Sentdex. Transforms the numpy arrays into Pytorch tensors. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SKfjXx61O2LU"
      },
      "outputs": [],
      "source": [
        "training_picture = torch.Tensor([i[0] for i in training_data]).view(-1, 100, 100)\n",
        "training_picture = training_picture/255.0\n",
        "training_class = torch.Tensor([i[1] for i in training_data])\n",
        "\n",
        "testing_picture = torch.Tensor([i[0] for i in testing_data]).view(-1, 100, 100)\n",
        "testing_picture = testing_picture/255.0\n",
        "testing_class = torch.Tensor([i[1] for i in testing_data])\n",
        "\n",
        "#Warning: \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
        "#How to fix/make more efficent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6EA_5x7pS8f"
      },
      "source": [
        "Adapted from Sentdex. The training and testing function. fwd_pass is used to pass the training data through the CNN and adjust for loss. The fwd_test function finds a random set of data of size size to test the CNN on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-FZL2LQPi0O"
      },
      "outputs": [],
      "source": [
        "def fwd_pass(net, training_data, testing_data, loss_function, optimizer, train=False):\n",
        "  #trains the function\n",
        "  if train:\n",
        "    net.zero_grad()\n",
        "    \n",
        "  outputs = net(training_data)\n",
        "  matches = [torch.argmax(i) == torch.argmax(j) for i, j in zip(outputs, testing_data)]\n",
        "  acc = matches.count(True)/len(matches)\n",
        "  loss = loss_function(outputs, testing_data)\n",
        "\n",
        "  if train:\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  return acc, loss\n",
        "\n",
        "def fwd_test(net, loss_function, optimizer, size = 32):\n",
        "  #size is the size of the random set of data that is used for the testing function\n",
        "  random_start = np.random.randint(len(testing_picture) - size)\n",
        "  test_pic, test_class = testing_picture[random_start: random_start + size], testing_class[random_start: random_start + size]\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    val_acc, val_loss = fwd_pass(net, test_pic.view(-1, 1, 100, 100).to(device), test_class.to(device), loss_function, optimizer)\n",
        "  \n",
        "  return val_acc, val_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03039V2ksaYK"
      },
      "source": [
        "Adapted from Sentdex and Ruben Winastwan. Rewritten by me to used Pandas and to fit our model. \n",
        "Train() trains the CNN with the passed arguments and writes on a log file with all of the testing data. \n",
        "create_acc_loss_graph() creates a graph of the training data accuracy vs. the testing accuracy and the training loss vs. the testing loss. \n",
        "Objective() creates a dictionary of hyperparameters to train the neural network on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFdyY861T9OV"
      },
      "outputs": [],
      "source": [
        "def train(param, MODEL_NAME = \"Default\"):\n",
        "  \"\"\"train() trains and tests the data and appends the data from the results into a csv file, with each training session identified using time.time()\n",
        "  param is a dictionary from the later objective() function.\"\"\"\n",
        "  if MODEL_NAME == \"Default\":\n",
        "    MODEL_NAME = f\"model-{int(time.time())}\" #creates model name based on the time the function is called, creates unique model names in the csv this way.\n",
        "  \n",
        "  net = Net().to(device) #creates network so that testing different conditions does not train on the same model\n",
        "  optimizer = getattr(optim, param['optimizer'])(net.parameters(), lr= param['learning_rate']) #optimize loss rate\n",
        "  loss_function = getattr(nn, param['loss_function'])()\n",
        "  labelbool = os.path.exists(\"/content/model.csv\")\n",
        "  \n",
        "  with open(\"model.csv\", \"a\") as f: #creates or opens a csv file to write the testing data on\n",
        "\n",
        "    if labelbool == False: #checks to see if file exists and if not, append the labels to the top\n",
        "      listlabels = [\"Model Name\", \"Time\", \"Train Accuracy\", \"Train Loss\", \"Test Accuracy\", \"Test Loss\"]\n",
        "      writer_object = csv.writer(f)\n",
        "      writer_object.writerow(listlabels) #appends labels to top of csv\n",
        "    \n",
        "    batch_size = param['batch_size'] #need to turn batch_size into int as param is float\n",
        "\n",
        "    for epoch in range(param['epochs']): #need to turn epochs into int as param is float\n",
        "      for i in tqdm(range(0, len(training_picture), batch_size)):\n",
        "          batch_pic = training_picture[i: i + batch_size].view(-1, 1, 100, 100)\n",
        "          batch_class = training_class[i: i + batch_size]\n",
        "          batch_pic, batch_class = batch_pic.to(device), batch_class.to(device)\n",
        "          acc, loss = fwd_pass(net, batch_pic, batch_class, loss_function, optimizer, True)\n",
        "          \n",
        "          if i % 5 == 0: #sample rate, how many iterations til the network is tested\n",
        "            val_acc, val_loss = fwd_test(net, loss_function, optimizer, size = 100) #<- change size for different size of testing sample\n",
        "            listvars = [MODEL_NAME, round(time.time(), 3), round(float(acc), 2), round(float(loss), 4), round(float(val_acc), 2), round(float(val_loss), 4)]\n",
        "            writer_object = csv.writer(f)\n",
        "            writer_object.writerow(listvars) #appends data to bottom of csv file\n",
        "      \n",
        "      print(f'Epoch {epoch + 1}')\n",
        "  \n",
        "  return MODEL_NAME, net #returns the model name so that it can be called in create_acc_loss_graph, and net to be utilized later\n",
        "\n",
        "matplotlib.style.use(\"ggplot\")\n",
        "\n",
        "def create_acc_loss_graph(model_name, graph = True):\n",
        "  \"\"\"create_acc_loss_graph creates a graph of the train vs. test accuracy and the train vs. test loss of a given model name.\"\"\"\n",
        "  try:\n",
        "    df = pd.read_csv(\"/content/model.csv\")\n",
        "    df = df.loc[df[\"Model Name\"] == model_name] #gets only the data associated with the model name\n",
        "  except:\n",
        "    raise Exception(\"Did not load model csv in yet\")\n",
        "  \n",
        "  if graph:\n",
        "    \n",
        "    fig = plt.figure()\n",
        "    ax1 = plt.subplot2grid((2, 1), (0, 0))\n",
        "    ax2 = plt.subplot2grid((2, 1), (1, 0), sharex = ax1)\n",
        "\n",
        "    df[\"Test Accuracy\"].plot(ax = ax1, title = model_name, legend = True)\n",
        "    df[\"Train Accuracy\"].plot(ax = ax1, legend = True)\n",
        "    df[\"Train Loss\"].plot(ax = ax2, legend = True)\n",
        "    df[\"Test Loss\"].plot(ax = ax2, legend = True)\n",
        "    \n",
        "    ax1.set_xlabel(\"Time\")\n",
        "    ax1.set_ylabel(\"Accuracy\")\n",
        "    ax2.set_xlabel(\"Time\")\n",
        "    ax2.set_ylabel(\"Loss\")\n",
        "\n",
        "    plt.show()\n",
        "  \n",
        "  if graph == False:\n",
        "    return df[\"Time\"].values.tolist(), df[\"Train Accuracy\"].values.tolist(), df[\"Train Loss\"].values.tolist(), df[\"Test Accuracy\"].values.tolist(), df[\"Test Loss\"].values.tolist()\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"objective() defines the hyperparameters being tested and trains the model based on a random set of hyperparameters defined in the params variable\"\"\"\n",
        "\n",
        "    params = {\n",
        "              'batch_size': trial.suggest_int('batch_size', 20, 100),\n",
        "              'learning_rate': trial.suggest_float('learning_rate', 1e-7, 1e-4),\n",
        "              'optimizer': trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]),\n",
        "              'loss_function': trial.suggest_categorical(\"loss_function\", [\"MSELoss\", \"CrossEntropyLoss\"]),\n",
        "              'epochs': trial.suggest_int('epochs', 5, 10)\n",
        "              }\n",
        "    #change the params dict to ranges one feels is necessary\n",
        "    \n",
        "    model_name = train(params)[0]\n",
        "\n",
        "    accuracy_list = create_acc_loss_graph(model_name, False)[1]\n",
        "    list_ind = len( accuracy_list) - int(len( accuracy_list) * .1)\n",
        "    accuracy_list = accuracy_list[list_ind:]\n",
        "    accuracy = sum(accuracy_list)/len(accuracy_list)\n",
        "\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted from Ruben Winastwan. Calls the study() function to test the hyperparameters and outputs the hyperparameters of the trial with the best accuracy."
      ],
      "metadata": {
        "id": "KH9NosjRYyz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
        "study.optimize(objective, n_trials=3)\n",
        "#n_trials is how many trials the objective() function tests randomized hyper parameters\n",
        "\n",
        "best_value = study.best_value\n",
        "best_trial = study.best_trial\n",
        "\n",
        "print(f'\\nBest Accuracy: {best_value}')\n",
        "print('\\nBest trial information:')\n",
        "for key, value in best_trial.params.items():\n",
        "    print(\"{}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "d7EeSCDOPe8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train() function without Optuna functionality."
      ],
      "metadata": {
        "id": "n1JwD4JEchEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(MODEL_NAME = f\"model-{int(time.time())}\", optimname = \"Adam\", lfname = \"MSELoss\", BATCH_SIZE = 50, EPOCHS = 5, learning_rate = 0.00008, save_model = False):\n",
        "  \"\"\"train() trains and tests the data and appends the data from the results into a csv file, with each training session identified using time.time()\n",
        "  if save_module is set to True, the model's weights will save to a .pt file.\"\"\"\n",
        "  net = Net().to(device)\n",
        "  optimizer = getattr(optim, optimname)(net.parameters(), lr= learning_rate) #optimize loss rate\n",
        "  loss_function = getattr(nn, lfname)()\n",
        "  loss_function = nn.MSELoss()\n",
        "  labelbool = os.path.exists(\"/content/model.csv\")\n",
        "  with open(\"model.csv\", \"a\") as f:\n",
        "    if labelbool == False: #checks to see if file exists and if not, append the labels to the top\n",
        "      listlabels = [\"Model Name\", \"Time\", \"Train Accuracy\", \"Train Loss\", \"Test Accuracy\", \"Test Loss\"]\n",
        "      writer_object = csv.writer(f)\n",
        "      writer_object.writerow(listlabels) #appends labels to top of csv\n",
        "    for epoch in range(EPOCHS):\n",
        "      for i in tqdm(range(0, len(training_picture), BATCH_SIZE)):\n",
        "          batch_pic = training_picture[i: i + BATCH_SIZE].view(-1, 1, 100, 100)\n",
        "          batch_class = training_class[i: i + BATCH_SIZE]\n",
        "          batch_pic, batch_class = batch_pic.to(device), batch_class.to(device)\n",
        "\n",
        "          acc, loss = fwd_pass(net, batch_pic, batch_class, loss_function, optimizer, True)\n",
        "          if i % 5 == 0: #sample rate, how many iterations til the network is tested\n",
        "            val_acc, val_loss = fwd_test(net, loss_function, optimizer, size = 100) #<- change size for different size of testing sample\n",
        "            listvars = [MODEL_NAME, round(time.time(), 3), round(float(acc), 2), round(float(loss), 4), round(float(val_acc), 2), round(float(val_loss), 4)]\n",
        "            writer_object = csv.writer(f)\n",
        "            writer_object.writerow(listvars) #appends data to bottom of csv file\n",
        "      print(f'Epoch {epoch + 1} of {EPOCHS}')\n",
        "  \n",
        "  if save_model:\n",
        "    torch.save(net.state_dict(), f\"/content/{MODEL_NAME}.pt\") #saves the CNN's weights to a .pt file\n",
        "\n",
        "  return MODEL_NAME, net"
      ],
      "metadata": {
        "id": "MuUaiQC-cgNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(EPOCHS = 2, save_model = True)"
      ],
      "metadata": {
        "id": "0YjrfdkMeCKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHBUMn3Stl8m"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHhK-FqFtcsc"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kVU45SVtlhT"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEW66hkAtk-Q"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W320hdQctgnT"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqGQY-pCtIUn"
      },
      "source": [
        "**Old Functions Below For Reference:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0m85ATzsy_Z"
      },
      "source": [
        "test_params() is used to test different parameters on its affect on accuracy for epochs, loss rate, and batch size. It outputs a graph of the testing variable vs. the accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZhchpLbj2Hu"
      },
      "outputs": [],
      "source": [
        "def test_params(BATCH_SIZE, EPOCHS, learning_rate, graph = True):\n",
        "  \"\"\"test_params tests the parameters of the train function. the args can be lists \"\"\"\n",
        "  if graph:\n",
        "    fig = plt.figure()\n",
        "    ax = plt.subplot2grid((2, 1), (0, 0))\n",
        "\n",
        "  if type(EPOCHS) == list and type(learning_rate) == float and type(BATCH_SIZE) == int:\n",
        "    list_epochs = []\n",
        "    list_accuracy = []\n",
        "    \n",
        "    for epoch in EPOCHS:\n",
        "      print(f'Testing Epoch: {epoch}')\n",
        "      list_epochs.append(epoch)\n",
        "      MODEL_NAME = f\"model-{int(time.time())}\"\n",
        "      print(f'Model Name {MODEL_NAME}')\n",
        "      train(MODEL_NAME, BATCH_SIZE, epoch, learning_rate)\n",
        "      times, accuracy, val_accs, losses, val_losses = create_acc_loss_graph(MODEL_NAME, False)\n",
        "      list_ind = len(times) - int(len(times) * .1)\n",
        "      val_accuracy = val_accs[list_ind:]\n",
        "      val_average = sum(val_accuracy)/len(val_accuracy)\n",
        "      list_accuracy.append(val_average)\n",
        "    \n",
        "    if graph:\n",
        "      ax.plot(list_epochs, list_accuracy)\n",
        "      ax.set_xlabel(\"Epochs\")\n",
        "      ax.set_ylabel(\"Accuracy\")\n",
        "    \n",
        "    return list_epochs, list_accuracy\n",
        "  \n",
        "  elif type(EPOCHS) == int and type(learning_rate) == list and type(BATCH_SIZE) == int:\n",
        "    list_learning_rate = []\n",
        "    list_accuracy = []\n",
        "    \n",
        "    for lr in learning_rate:\n",
        "      print(f'Testing Loss Rate: {lr}')\n",
        "      list_learning_rate.append(lr)\n",
        "      MODEL_NAME = f\"model-{int(time.time())}\"\n",
        "      print(f'Model Name {MODEL_NAME}')\n",
        "      train(MODEL_NAME, BATCH_SIZE, EPOCHS, lr)\n",
        "      times, accuracy, val_accs, losses, val_losses = create_acc_loss_graph(MODEL_NAME, False)\n",
        "      list_ind = len(times) - int(len(times) * .1)\n",
        "      val_accuracy = val_accs[list_ind:]\n",
        "      val_average = sum(val_accuracy)/len(val_accuracy)\n",
        "      list_accuracy.append(val_average)\n",
        "    \n",
        "    if graph:\n",
        "      ax.plot(list_learning_rate, list_accuracy)\n",
        "      ax.set_xlabel(\"Loss Rates\")\n",
        "      ax.set_ylabel(\"Accuracy\")\n",
        "    \n",
        "    return list_learning_rate, list_accuracy\n",
        "  \n",
        "  elif type(EPOCHS) == int and type(learning_rate) == float and type(BATCH_SIZE) == list:\n",
        "    list_batch_size = []\n",
        "    list_accuracy = []\n",
        "    \n",
        "    for size in BATCH_SIZE:\n",
        "      print(f'Testing Batch Size: {size}')\n",
        "      list_batch_size.append(size)\n",
        "      MODEL_NAME = f\"model-{int(time.time())}\"\n",
        "      print(f'Model Name {MODEL_NAME}')\n",
        "      train(MODEL_NAME, size, EPOCHS, learning_rate)\n",
        "      times, accuracy, val_accs, losses, val_losses = create_acc_loss_graph(MODEL_NAME, False)\n",
        "      list_ind = len(times) - int(len(times) * .1)\n",
        "      val_accuracy = val_accs[list_ind:]\n",
        "      val_average = sum(val_accuracy)/len(val_accuracy)\n",
        "      list_accuracy.append(val_average)\n",
        "    \n",
        "    if graph:\n",
        "      ax.plot(list_batch_size, list_accuracy)\n",
        "      ax.set_xlabel(\"Batch Sizes\")\n",
        "      ax.set_ylabel(\"Accuracy\")\n",
        "    return list_batch_size, list_accuracy\n",
        "  \n",
        "  elif type(EPOCHS) == list and type(learning_rate) == list and type(BATCH_SIZE) == int:\n",
        "    list_epochs = []\n",
        "    list_lr = []\n",
        "    list_accuracy = []\n",
        "    ax3d = plt.axes(projection='3d')\n",
        "    for epoch in EPOCHS:\n",
        "      for lr in learning_rate:\n",
        "        print(f'Testing Epochs: {epoch}')\n",
        "        print(f'Testing Learning Rate: {lr}')\n",
        "        list_epochs.append(epoch)\n",
        "        list_lr.append(lr)\n",
        "        MODEL_NAME = f\"model-{int(time.time())}\"\n",
        "        print(f'Model Name {MODEL_NAME}')\n",
        "        train(MODEL_NAME, BATCH_SIZE, epoch, lr)\n",
        "        times, accuracy, val_accs, losses, val_losses = create_acc_loss_graph(MODEL_NAME, False)\n",
        "        list_ind = len(times) - int(len(times) * .1)\n",
        "        val_accuracy = val_accs[list_ind:]\n",
        "        val_average = sum(val_accuracy)/len(val_accuracy)\n",
        "        list_accuracy.append(val_average)\n",
        "    \n",
        "    if graph:\n",
        "      ax3d.scatter3d(list_epochs, list_lr, list_accuracy)\n",
        "      ax3d.set_xlabel(\"Epochs\")\n",
        "      ax3d.set_ylabl(\"Learning Rates\")\n",
        "      ax3d.set_zlabel(\"Accuracy\")\n",
        "    return list_epochs, list_lr, list_accuracy\n",
        "\n",
        "  elif type(EPOCHS) == int and type(learning_rate) == float and type(BATCH_SIZE) == int:\n",
        "    MODEL_NAME = f\"model-{int(time.time())}\"\n",
        "    print(f'Model Name {MODEL_NAME}')\n",
        "    train(MODEL_NAME, BATCH_SIZE, EPOCHS, learning_rate)\n",
        "    times, accuracy, val_accs, losses, val_losses = create_acc_loss_graph(MODEL_NAME, False)\n",
        "    list_ind = len(times) - int(len(times) * .1)\n",
        "    val_accuracy = val_accs[list_ind:]\n",
        "    val_average = sum(val_accuracy)/len(val_accuracy)\n",
        "    \n",
        "    return val_average\n",
        "  \n",
        "  else: \n",
        "    raise TypeError(\"Please Input the Correct Types\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "chbnWBzMtrYH"
      },
      "outputs": [],
      "source": [
        "#optimal learning rate is approximately 0.00006 per current testing\n",
        "epoch_list = [1, 3, 5]\n",
        "test_params(200, epoch_list, 0.00006, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL0yPzQj-9BY"
      },
      "outputs": [],
      "source": [
        "model_name = (train(BATCH_SIZE = 50, EPOCHS = 10))[0]\n",
        "create_acc_loss_graph(model_name, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQvy_gKSO4IM"
      },
      "outputs": [],
      "source": [
        "#Adapted from Sentdex\n",
        "\n",
        "testing_picture.to(device)\n",
        "testing_class.to(device)\n",
        "\n",
        "def train(net, BATCH_SIZE = 100, EPOCHS = 10): #optimize parametes\n",
        "  optimizer = optim.Adam(net.parameters(), lr = 0.00008) #optimize loss rate\n",
        "  loss_function = nn.MSELoss()\n",
        "  for epoch in range(EPOCHS):\n",
        "      for i in tqdm(range(0, len(training_picture), BATCH_SIZE)):\n",
        "          batch_pic = training_picture[i: i + BATCH_SIZE].view(-1, 1, 100, 100)\n",
        "          batch_class = training_class[i: i + BATCH_SIZE]\n",
        "          \n",
        "          batch_pic, batch_class = batch_pic.to(device), batch_class.to(device) \n",
        "          \n",
        "          net.zero_grad()\n",
        "          outputs = net(batch_pic)\n",
        "          loss = loss_function(outputs, batch_class)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "      print(f'Epoch: {epoch}, Loss: {loss}')\n",
        "\n",
        "testing_picture = testing_picture[0:5000]\n",
        "testing_class = testing_class[0:5000]\n",
        "\n",
        "def test(net):\n",
        "  correct, total = 0, 0\n",
        "  with torch.no_grad():\n",
        "      for i in tqdm(range(len(testing_picture))):\n",
        "          real_class = torch.argmax(testing_class[i]).to(device)\n",
        "          net_out = net(testing_picture[i].view(-1, 1, 100, 100).to(device))[0]\n",
        "          predicted_class = torch.argmax(net_out)\n",
        "          #print(f'{predicted_class}, {real_class}')\n",
        "          if predicted_class == real_class:\n",
        "              correct += 1\n",
        "          total += 1\n",
        "  print(correct, total)\n",
        "  return(round(correct/total, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ilRryfmPfyz"
      },
      "outputs": [],
      "source": [
        "net = Net().to(device)\n",
        "train(net, BATCH_SIZE = 100, EPOCHS = 5)\n",
        "print(test(net))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}