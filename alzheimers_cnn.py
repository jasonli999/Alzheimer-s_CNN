# -*- coding: utf-8 -*-
"""Alzheimers_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jFgPIEw7db12Z7gBsOxsZX3C7wJ9wxtY

Credit to sentdex and his Youtube PyTorch tutorials for the basic CNN Model that we adapted.

Import Statements are below:
"""

import pandas as pd
import numpy as np
import cv2
from tqdm import tqdm
import os 
import torch 
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

"""Checks to see if a GPU is available for the model to run on:"""

if torch.cuda.is_available():
  device = torch.device("cuda:0")
else:
  device = torch.device("cpu")

print(f'Device is: {device}')

"""Downloads and unzips the Kaggle dataset into the local directory:"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# os.environ['KAGGLE_CONFIG_DIR'] = "/content"
# !kaggle datasets download -d uraninjo/augmented-alzheimer-mri-dataset
# !unzip "/content/augmented-alzheimer-mri-dataset.zip"
# #downloads and unzips the dataset from Kaggle. Note that the kaggle.json file will need to be uploaded to the Colab for this to work

"""Functions to pre-process the data. folder_to_array() is mainly used in the model to turn the images into numpy arrays and to classify the images."""

def folder_to_csv(folder, classification, save = False):
    """Folder is a folder path, classification is the string classifiction. Save is a boolean
    folder_to_csv() returns a csv with the path of the images in the first column and the classification in the second column
    setting 'save' to true saves the csv to your drive with the name of the file as the name of the classification variable"""
    file_names = []
    for picture in os.listdir(folder):
      path = os.path.join(folder, picture)
      file_names.append([path, classification])
      df = pd.DataFrame(file_names, columns = ["File Path", "Classification"])
    if save == True:
        name = f'{classification}.csv'
        csv = df.to_csv(name, index = False)
    return df.to_csv(index = False)

def folder_to_array(folder, vectorlocation, image_size, save = False):
    """folder is a folder path, image_size is the image size of the post-processed picture, vectorlocation is the index of the one-hot vector
    folder_to_array() returns all of the images in the folder specified into an 2d array with column1 as the image array and column2 as the
    one-hot vector representing the data classification"""
    training_data = []
    for picture in tqdm(os.listdir(folder)):
        if "jpg" in picture:
            path = os.path.join(folder, picture)
            img = cv2.imread(path)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            img = cv2.resize(img, (image_size, image_size))
            training_data.append([np.array(img), np.eye(4)[vectorlocation]]) #<- second index is the one-hot vector
    training_data = np.array(training_data, list)
    if save == True:
        np.save(folder, training_data)
    return training_data

"""Calls the folder_to_array() functions and creates the training and testing data arrays."""

nondemented_array = folder_to_array("/content/AugmentedAlzheimerDataset/NonDemented", 0, 100)
verymilddemented_array = folder_to_array("/content/AugmentedAlzheimerDataset/VeryMildDemented", 1, 100)
milddemented_array = folder_to_array("/content/AugmentedAlzheimerDataset/MildDemented", 2, 100)
moderatedemented_array = folder_to_array("/content/AugmentedAlzheimerDataset/ModerateDemented", 3, 100)
#creates the arrays from the unzipped Kaggle data located in the Colab

#train_size = int(min(len(nondemented_array), len(verymilddemented_array), len(milddemented_array), len(moderatedemented_array)) / 2)
#the size of each sample size in the training batch is half of the minimum sample size

train_size = 1000 #try about ~20% of data in training instead

training_data = np.concatenate((nondemented_array[0: train_size], verymilddemented_array[0: train_size], milddemented_array[0: train_size], moderatedemented_array[0: train_size]))
testing_data = np.concatenate((nondemented_array[train_size:], verymilddemented_array[train_size:], milddemented_array[train_size:], moderatedemented_array[train_size:]))
#creates the training data by concatenating all of the arrays of length train_size together, creates testing data by concatenating the rest of the data
#generally we want a smaller portion of the data to be a part of the training dataset to avoid overfitting
np.random.shuffle(training_data)
np.random.shuffle(testing_data)
#Shuffling and randomizing the data

training_data = np.array(training_data)
testing_data = np.array(testing_data)

"""The framework for the CNN, which consists of Convolutional layers for processing the image, Max pooling layers which downscales the 2D convolutional data into 1D, and the Fully connected layers, which are the neural network."""

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        #optimize number of layers, hypertune parameters
        self.conv1 = nn.Conv2d(1, 32, 5)
        self.conv2 = nn.Conv2d(32, 64, 5)
        self.conv3 = nn.Conv2d(64, 128, 5)
        self.pool1 = nn.MaxPool2d((2, 2))
        self.pool2 = nn.MaxPool2d((2, 2))
        self.pool3 = nn.MaxPool2d((2, 2))
        self.fc1 = nn.Linear(10368, 10368) #size fo the fully connected layer was determined by the size of the pictures, which are 180 x 180 pixels
        self.fc2 = nn.Linear(10368, 4)
        
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        x = F.relu(self.conv2(x))
        x = self.pool2(x)
        x = F.relu(self.conv3(x))
        x = self.pool3(x)
        x = x.flatten(start_dim=1) # flattening out
        #print(x.shape)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

net = Net().to(device)
#net.forward(torch.randn(1, 1, 100, 100))

"""Transforms the numpy arrays into Pytorch tensors. """

training_picture = torch.Tensor([i[0] for i in training_data]).view(-1, 100, 100)
training_picture = training_picture/255.0
training_class = torch.Tensor([i[1] for i in training_data])

testing_picture = torch.Tensor([i[0] for i in testing_data]).view(-1, 100, 100)
testing_picture = testing_picture/255.0
testing_class = torch.Tensor([i[1] for i in testing_data])

#Warning: "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)
#How to fix/make more efficent?

"""The training and testing functions:"""

testing_picture.to(device)
testing_class.to(device)

def train(net, BATCH_SIZE = 100, EPOCHS = 10): #optimize parametes

  for epoch in range(EPOCHS):
      optimizer = optim.Adam(net.parameters(), lr = 0.0001) #loss rate
      loss_function = nn.MSELoss()
      for i in tqdm(range(0, len(training_picture), BATCH_SIZE)):
          batch_pic = training_picture[i: i + BATCH_SIZE].view(-1, 1, 100, 100)
          batch_class = training_class[i: i + BATCH_SIZE]
          
          batch_pic, batch_class = batch_pic.to(device), batch_class.to(device) 

          net.zero_grad()
          outputs = net(batch_pic)
          loss = loss_function(outputs, batch_class)
          loss.backward()
          optimizer.step()
      print(f'Epoch: {epoch}, Loss: {loss}')

def test(net):
  correct, total = 0, 0
  with torch.no_grad():
      for i in tqdm(range(len(testing_picture))):
          real_class = torch.argmax(testing_class[i]).to(device)
          net_out = net(testing_picture[i].view(-1, 1, 100, 100).to(device))[0]
          predicted_class = torch.argmax(net_out)
          #print(f'{predicted_class}, {real_class}')
          if predicted_class == real_class:
              correct += 1
          total += 1
  print(correct, total)
  return(round(correct/total, 3))

train(net, BATCH_SIZE = 100, EPOCHS = 10)
print(test(net))

import pandas as pd
import matplotlib as plt
data = []
for epochs in range(50):
  #d = {"Epochs": epochs, "Accuracy": accuracy}
  train(net, BATCH_SIZE = 100, EPOCHS = epochs)
  accuracy = test(net)
  data.append([epochs, accuracy])
df = pd.DataFrame(data)
df["epochs"].hist(bins = 25)
df.plot.scatter(x = "Epochs", y = "Accuracy")